# ============================================
# Intellibooks Studio - Environment Configuration
# ============================================

# LLM Provider API Keys
OPENAI_API_KEY=your-openai-api-key-here
ANTHROPIC_API_KEY=your-anthropic-api-key-here
OPENROUTER_API_KEY=xyz

# Default LLM Provider (openai, anthropic, or openrouter)
DEFAULT_LLM_PROVIDER=openrouter

# OpenRouter Model (used when provider is openrouter)
OPENROUTER_MODEL=anthropic/claude-sonnet-4

# Database Configuration
DATABASE_URL=sqlite:///./data/transcriptions.db

# Audio Processing
AUDIO_STORAGE_PATH=./data/audio
MAX_AUDIO_SIZE_MB=100

# Whisper Model (tiny, base, small, medium, large)
WHISPER_MODEL=small

# Whisper.cpp Configuration (local transcription)
WHISPER_PATH=C:\Users\JIPL\whisper.cpp\build\bin\Release\whisper-cli.exe
WHISPER_MODEL_PATH=C:\Users\JIPL\whisper.cpp\models\ggml-small.bin

# ============================================
# RAG Pipeline Configuration
# ============================================

# ChromaDB (Docker)
CHROMA_HOST=localhost
CHROMA_PORT=8000
CHROMA_COLLECTION=intellibooks_knowledge

# Embedding Model
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Document Processing
RAG_CHUNK_SIZE=1000
RAG_CHUNK_OVERLAP=200
RAG_MAX_WORKERS=4

# Ray (set to true to enable distributed processing via Docker)
USE_RAY=true
RAY_ADDRESS=ray://localhost:10001
RAY_NUM_CPUS=4

# RabbitMQ (set to true to enable async task queue via Docker)
USE_RABBITMQ=true
RABBITMQ_HOST=localhost
RABBITMQ_PORT=5672
RABBITMQ_USER=admin
RABBITMQ_PASSWORD=devpassword123

# ============================================
# Other Services
# ============================================

# MCP Server Configuration
MCP_SERVER_HOST=localhost
MCP_SERVER_PORT=8765

# GitHub Integration (optional, for future use)
GITHUB_TOKEN=your-github-token-here
GITHUB_REPO=owner/repo-name

# Logging
LOG_LEVEL=INFO
